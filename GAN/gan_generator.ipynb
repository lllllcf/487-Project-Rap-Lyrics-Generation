{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data_utils import merge_lists, gen_clean\n",
    "\n",
    "with open(os.path.join('../data/data_lyrics.json'), 'r') as fp:\n",
    "    lyrics_data = json.load(fp)\n",
    "with open(os.path.join('../data/data_news.json'), 'r') as fp:\n",
    "    news_data = json.load(fp)\n",
    "with open(os.path.join('../data/data_rap.json'), 'r') as fp:\n",
    "    raw_rap_data = json.load(fp)\n",
    "\n",
    "gen_clean_control = {'lemmatize': True, 'stop_words': False, 'remove_number': True,}\n",
    "all_data = gen_clean(merge_lists(lyrics_data) + merge_lists(news_data) + merge_lists(raw_rap_data), gen_clean_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from data_utils import add_some_music, gen_pre_data_preprocession, add_some_news, dis_pre_data_preprocession, get_dev_data\n",
    "\n",
    "gen_percentage = 0.7 # percentage for generator pretraining from rap data\n",
    "music_percentage = 0.1 # percentage of music added into rap lyrics\n",
    "\n",
    "# generator data preprocessing\n",
    "rap_music = add_some_music(raw_rap_data, lyrics_data, music_percentage)\n",
    "final_train, gen_pre, dis_rap_raw = gen_pre_data_preprocession(rap_music, gen_percentage, gen_clean_control)\n",
    "\n",
    "# discriminator data preprocessing\n",
    "rap_news = add_some_news(dis_rap_raw, news_data)\n",
    "dis_pre = dis_pre_data_preprocession(rap_news)\n",
    "\n",
    "# get validation data\n",
    "pre_dev_percentage = 0.1\n",
    "gen_pre, gen_pre_dev = get_dev_data(gen_pre, pre_dev_percentage)\n",
    "dis_pre, dis_pre_dev = get_dev_data(dis_pre, pre_dev_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sen_embed = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import GENDataset, DISDataset, basic_collate_fn\n",
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--pre-train-epochs', type=int, default=30)\n",
    "# parser.add_argument('--batch-size', type=int, default=256)\n",
    "# parser.add_argument('--sequence-length', type=int, default=5)\n",
    "# gen_args = parser.parse_args()\n",
    "\n",
    "gen_args = {\"pre_train_epochs\": 10, \"batch_size\": 64, \"sequence_length\": 10}\n",
    "\n",
    "# new_parser = argparse.ArgumentParser()\n",
    "# new_parser.add_argument('--pre-train-epochs', type=int, default=30)\n",
    "# dis_args = new_parser.parse_args()\n",
    "\n",
    "dis_args = {\"pre_train_epochs\": 1}\n",
    "\n",
    "dis_batch_size = 32\n",
    "\n",
    "# gen_pre = gen_pre[0:512]\n",
    "# gen_pre_dev = gen_pre\n",
    "# dis_pre_dev = dis_pre[0:256] + dis_pre[-256:]\n",
    "# final_train = gen_pre\n",
    "# dis_pre = dis_pre[0:256] + dis_pre[-256:]\n",
    "\n",
    "gen_pre_data = GENDataset(gen_args, gen_pre, all_data)\n",
    "gen_pre_dev_data = GENDataset(gen_args, gen_pre_dev, all_data)\n",
    "final_train_data = GENDataset(gen_args, final_train, all_data)\n",
    "\n",
    "dis_pre_data = DISDataset(dis_pre, sen_embed)\n",
    "dis_pre_dev_data = DISDataset(dis_pre_dev, sen_embed)\n",
    "\n",
    "gen_dataloader = DataLoader(gen_pre_data, batch_size=gen_args[\"batch_size\"])\n",
    "gen_dev_loader = DataLoader(gen_pre_dev_data, batch_size=1)\n",
    "final_loader = DataLoader(final_train_data, batch_size=1)\n",
    "\n",
    "dis_dataloader = DataLoader(dis_pre_data, batch_size=dis_batch_size, collate_fn=basic_collate_fn, shuffle=True)\n",
    "dis_dev_loader = DataLoader(dis_pre_dev_data, batch_size=gen_args[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model.generator import Generator\n",
    "from model.discriminator import Discriminator\n",
    "\n",
    "lstm_input_size, num_layers, lstm_hidden_dim, dropout = 128, 2, 256, 0.1\n",
    "dis_hidden_dim = 1024\n",
    "\n",
    "generator = Generator(gen_pre_data, lstm_input_size, num_layers, lstm_hidden_dim, dropout)\n",
    "discriminator = Discriminator(dis_hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'batch': 0, 'loss': 11.137024879455566}\n",
      "{'epoch': 0, 'batch': 1, 'loss': 11.124754905700684}\n",
      "{'epoch': 0, 'batch': 0, 'loss': 0.7305691242218018}\n",
      "{'epoch': 0, 'batch': 1, 'loss': 0.6041577458381653}\n",
      "{'epoch': 0, 'batch': 2, 'loss': 1.2411816120147705}\n",
      "{'epoch': 0, 'batch': 3, 'loss': 0.7519977688789368}\n",
      "{'epoch': 0, 'batch': 4, 'loss': 1.626331090927124}\n",
      "{'epoch': 0, 'batch': 5, 'loss': 0.8829678893089294}\n",
      "{'epoch': 0, 'batch': 6, 'loss': 0.7123408317565918}\n",
      "{'epoch': 0, 'batch': 7, 'loss': 0.8326322436332703}\n",
      "{'epoch': 0, 'batch': 8, 'loss': 1.1079089641571045}\n",
      "{'epoch': 0, 'batch': 9, 'loss': 0.8852540254592896}\n",
      "{'epoch': 0, 'batch': 10, 'loss': 0.7931972742080688}\n",
      "{'epoch': 0, 'batch': 11, 'loss': 0.6652122735977173}\n",
      "{'epoch': 0, 'batch': 12, 'loss': 0.847244381904602}\n",
      "{'epoch': 0, 'batch': 13, 'loss': 0.8188858032226562}\n",
      "{'epoch': 0, 'batch': 14, 'loss': 0.7762765288352966}\n",
      "{'epoch': 0, 'batch': 15, 'loss': 0.753746509552002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chengfan Li\\Desktop\\487-Project-Rap-Lyrics-Generation\\GAN\\train_utils.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(res, torch.tensor(y).float())\n",
      "c:\\Users\\Chengfan Li\\Desktop\\487-Project-Rap-Lyrics-Generation\\GAN\\train_utils.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred.append(torch.tensor(res))\n"
     ]
    }
   ],
   "source": [
    "from train import pre_train_generator, pre_train_discriminator\n",
    "\n",
    "device = 'cpu'\n",
    "pre_patience = 10\n",
    "\n",
    "gen_loss_type, gen_optim_type = 'cross', 'adam'\n",
    "g_lr, g_weight_decay = 0.001, 0.00001\n",
    "\n",
    "dis_loss_type, dis_optim_type = 'bce', 'adam'\n",
    "d_lr, d_weight_decay = 0.001, 0.00001\n",
    "\n",
    "generator = pre_train_generator(gen_args, generator, gen_dataloader, gen_dev_loader, gen_loss_type, gen_optim_type, g_lr, g_weight_decay, pre_patience, device)\n",
    "discriminator = pre_train_discriminator(dis_args, discriminator, dis_dataloader, dis_dev_loader, dis_loss_type, dis_optim_type, d_lr, d_weight_decay, pre_patience, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_hyper_parameters():\n",
    "    _g_para_list = [{\"optim_type\": 'adam', 'lr': 0.01, \"weight_decay\": 1e-4}]\n",
    "    _d_para_list = [{\"optim_type\": 'adam', 'lr': 0.01, \"weight_decay\": 1e-4}]\n",
    "    _num_epoch = 40\n",
    "    _patience = 10\n",
    "    _max_words = 10\n",
    "    _device = 'cpu'\n",
    "    return _g_para_list, _d_para_list, _num_epoch, _patience, _max_words, _device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ Start Training ------------------------\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CHENGF~1\\AppData\\Local\\Temp/ipykernel_25500/2854480331.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mbest_dis_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_gen_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg_para\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_para\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_para_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_para_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_train_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_para\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_para\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_dev_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# update best parameters if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chengfan Li\\Desktop\\487-Project-Rap-Lyrics-Generation\\GAN\\train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(generator, discriminator, input_loader, input_data, num_epoch, g_para, d_para, val_loader, patience, max_words, device)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0msen_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_sen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mnum_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mgenerated_lyrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_rap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msen_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mdiscriminator_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen_embed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_lyrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chengfan Li\\Desktop\\487-Project-Rap-Lyrics-Generation\\GAN\\generate_rap.py\u001b[0m in \u001b[0;36mgenerate_rap\u001b[1;34m(generator, sen_input, num_sentences, max_words, dataset)\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mword_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mstop\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0msen_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0msen_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from train import train_model\n",
    "from data_utils import plot_loss\n",
    "from generate_rap import generate_rap\n",
    "import numpy as np\n",
    "\n",
    "g_para_list, d_para_list, num_epoch, patience, max_words, device = get_hyper_parameters()\n",
    "\n",
    "# model training\n",
    "best_gen, best_dis, best_stats = copy.deepcopy(generator), copy.deepcopy(discriminator), None\n",
    "best_lr, best_wd, best_bs, best_hd, best_lt, best_om = 0, 0, 0, 0, '', ''\n",
    "best_dis_loss, best_gen_loss = float('-inf'), float('inf')\n",
    "for g_para, d_para in itertools.product(g_para_list, d_para_list):\n",
    "    g, d, stats = train_model(generator, discriminator, final_loader, final_train_data, num_epoch, g_para, d_para, gen_dev_loader, patience, max_words, device)\n",
    "\n",
    "    # update best parameters if needed\n",
    "    if np.mean(stats['dis_loss']) > best_dis_loss and np.mean(stats['gen_loss']) < best_gen_loss:\n",
    "        best_dis_loss = np.mean(stats['dis_loss'])\n",
    "        best_gen_loss = np.mean(stats['gen_loss'])\n",
    "        best_gen, best_dis, best_stats = copy.deepcopy(g), copy.deepcopy(d) , copy.deepcopy(stats)\n",
    "        best_g_para, best_d_para = g_para, d_para\n",
    "\n",
    "    print(\"\\n\\nBest hidden dimension: {}, Best learning rate: {}, best weight_decay: {}, best batch_size: {}, best loss type： {}, best optimizer: {}\".format(\n",
    "    best_hd, best_lr, best_wd, best_bs, best_lt, best_om))\n",
    "print(\"Generator loss: {:.4f}\".format(best_gen_loss))\n",
    "print(\"Discriminator loss: {:.4f}\".format(best_dis_loss))\n",
    "plot_loss(best_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "i build a castle nickel drainage digit simile whatsoever extra budding bisque tee moan oracle.\n",
      "emotionless sprightly philosophical shelf gynecology gleeful stalk nob vive.\n",
      "attain turk quizzical adroit hysterical wonderfully define acolyte grasping nope blas.\n",
      "plus singled bullock clan moderation craniopagus bandit cylindrical enormously.\n",
      "brimstone golfer stardom seesaw stringent quart unheated catty backfire maidan herse.\n",
      "kitten stagnation honest huh till fluid meadow morello coarseness indelible.\n",
      "interior tractor pedophilia anticipate migraine affirmation cutlet resin.\n",
      "substandard diminishment redouble sidewalk derrick vibrantly uniting disrespect.\n",
      "hump cureless chub councilman pew orthodontics scraped populace potent rightist.\n",
      "quorum hymen afterglow manicure hypochondriac crisscross nightfall attention schematic utopian.\n"
     ]
    }
   ],
   "source": [
    "from generate_rap import generate_rap\n",
    "\n",
    "sen_input = \"i build a castle\"\n",
    "num_sentences = 10\n",
    "max_words = 10\n",
    "\n",
    "lyrics = generate_rap(best_gen, sen_input, num_sentences, max_words, final_train_data)\n",
    "for sen in lyrics:\n",
    "    print(sen + '.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
